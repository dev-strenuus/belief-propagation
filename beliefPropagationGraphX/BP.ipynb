{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Message\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Message(source: Long, destination: Long, value: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileName = ../data/input.txt\n",
       "lines = Array(5 5, 0 1, 0.25 0.25 0.25 0.25, 1 2 3, 0.16706443914081145 0.01909307875894988 0.18138424821002386 0.1431980906921241 0.02386634844868735 0.15513126491646778 0.07159904534606205 0.2386634844868735, 2, 0.4 0.6, 3 4, 0.20 0.30 0.40 0.10, 3, 0.25 0.75)\n",
       "variablesNumber = 5\n",
       "factorsNumber = 5\n",
       "variablesNumber = 5\n",
       "factorsNumber = 5\n",
       "tempNodes = Array((0,(0,0,null)), (1,(0,0,null)), (2,(0,0,null)), (3,(0,0,null)), (4,(0,0,null)), (5,(1,0,Array(0.25, 0.25, 0.25, 0.25))), (6,(1,0,Array(0.16706443914081145, 0.01909307875894988, 0.18138424821002386, 0.1431980906921241, 0.02386634844868735, 0.15513126491646778, 0.07159904534606205, 0.2386634844868735))), (7,(1,0,Array(0.4,...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array((0,(0,0,null)), (1,(0,0,null)), (2,(0,0,null)), (3,(0,0,null)), (4,(0,0,null)), (5,(1,0,Array(0.25, 0.25, 0.25, 0.25))), (6,(1,0,Array(0.16706443914081145, 0.01909307875894988, 0.18138424821002386, 0.1431980906921241, 0.02386634844868735, 0.15513126491646778, 0.07159904534606205, 0.2386634844868735))), (7,(1,0,Array(0.4,..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fileName = \"../data/input.txt\"\n",
    "var lines = sc.textFile(fileName).collect()\n",
    "var variablesNumber: Long = 0\n",
    "var factorsNumber: Long = 0\n",
    "variablesNumber = lines(0).split(\" \")(0).toLong\n",
    "factorsNumber = lines(0).split(\" \")(1).toLong\n",
    "var tempNodes: Array[(Long, (Int, Int, Array[Double]))] = new Array[(Long, (Int, Int,  Array[Double]))]((variablesNumber+factorsNumber).toInt) // (id, (type, leaf, distribution))\n",
    "for (a <- 0 to (variablesNumber+factorsNumber - 1).toInt){\n",
    "    tempNodes(a) = (a.toLong, (0,0, null))\n",
    "}\n",
    "var tempEdges: Array[Edge[Long]] = new Array[Edge[Long]](((variablesNumber+factorsNumber-1)*2).toInt)\n",
    "var cont = 0\n",
    "for (a <- 1 to lines.size-1){\n",
    "    if(a%2 == 1){\n",
    "        val factorNode = variablesNumber + ((a-1)/2).toLong\n",
    "        var pos = 0\n",
    "        for (variableNode <- lines(a).split(\" \")){\n",
    "            tempEdges(cont) = Edge(variableNode.toLong, factorNode, 0)\n",
    "            cont = cont + 1\n",
    "            tempEdges(cont) = Edge(factorNode, variableNode.toLong, pos)\n",
    "            cont = cont + 1\n",
    "            pos = pos + 1\n",
    "        } \n",
    "    }else{\n",
    "        val distribution:  Array[Double] = lines(a).split(\" \").map(_.toDouble)\n",
    "        val factorNode = (variablesNumber + ((a-1)/2).toLong).toInt\n",
    "        tempNodes(factorNode) = (factorNode, (1, 0, distribution))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nodes = ParallelCollectionRDD[112] at parallelize at <console>:37\n",
       "edges = ParallelCollectionRDD[113] at parallelize at <console>:38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[113] at parallelize at <console>:38"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nodes: RDD[(VertexId, (Int, Int,  Array[Double]))] = sc.parallelize(tempNodes)\n",
    "val edges: RDD[Edge[Long]] = sc.parallelize(tempEdges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:35: error: overloaded method value ofDim with alternatives:\n",
       "  (n1: Int,n2: Int,n3: Int,n4: Int,n5: Int)(implicit evidence$7: scala.reflect.ClassTag[Int])Array[Array[Array[Array[Array[Int]]]]] <and>\n",
       "  (n1: Int,n2: Int,n3: Int,n4: Int)(implicit evidence$6: scala.reflect.ClassTag[Int])Array[Array[Array[Array[Int]]]] <and>\n",
       "  (n1: Int,n2: Int,n3: Int)(implicit evidence$5: scala.reflect.ClassTag[Int])Array[Array[Array[Int]]] <and>\n",
       "  (n1: Int,n2: Int)(implicit evidence$4: scala.reflect.ClassTag[Int])Array[Array[Int]] <and>\n",
       "  (n1: Int)(implicit evidence$3: scala.reflect.ClassTag[Int])Array[Int]\n",
       " cannot be applied to ((Int, Int))\n",
       "       val matrix = Array.ofDim[Int](dimensions)\n",
       "                               ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dimensions = (2,2)\n",
    "val matrix = Array.ofDim[Int](dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "6\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "initialMsg = -9999\n",
       "vertices = ParallelCollectionRDD[0] at parallelize at <console>:36\n",
       "relationships = ParallelCollectionRDD[1] at parallelize at <console>:38\n",
       "graph = org.apache.spark.graphx.impl.GraphImpl@674c5d8a\n",
       "minGraph = org.apache.spark.graphx.impl.GraphImpl@2e9b2086\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "mergeMsg: (msg1: Int, msg2: Int)Int\n",
       "vprog: (vertexId: org.apache.spark.graphx.VertexId, value: (Int, Int), message: Int)(Int, Int)\n",
       "sendMsg: (triplet: org.apache.spark.graphx.EdgeTriplet[(Int, Int),Boolean])Iterator[(org.apache.spark.graphx.VertexId, Int)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@2e9b2086"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.4 0.6\n",
      "3 4\n",
      "0.20 0.30 0.40 0.10\n",
      "3\n",
      "0.25 0.75\n",
      "5 5\n",
      "0 1\n",
      "0.25 0.25 0.25 0.25\n",
      "1 2 3\n",
      "0.16706443914081145 0.01909307875894988 0.18138424821002386 0.1431980906921241 0.02386634844868735 0.15513126491646778 0.07159904534606205 0.2386634844868735\n"
     ]
    }
   ],
   "source": [
    "val initialMsg = -9999\n",
    "// (vertexID, (new vertex value, old vertex value))\n",
    "val vertices: RDD[(VertexId, (Int, Int))] = sc.parallelize(Array((1L, (1, -1)),\n",
    "  (2L, (2, -1)), (3L, (3, -1)), (6L, (6, -1))))\n",
    "val relationships: RDD[Edge[Boolean]] = sc.parallelize(Array(Edge(1L, 2L, true),\n",
    "  Edge(2L, 1L, true), Edge(2L, 6L, true), Edge(3L, 6L, true), Edge(6L, 1L, true),\n",
    "  Edge(6L, 3L, true)))\n",
    "val graph = Graph(vertices, relationships)\n",
    "\n",
    "def mergeMsg(msg1: Int, msg2: Int): Int = math.max(msg1, msg2)\n",
    "\n",
    "// Apply: the function for receiving messages\n",
    "def vprog(vertexId: VertexId, value: (Int, Int), message: Int): (Int, Int) = {\n",
    "  if (message == initialMsg) // superstep 0\n",
    "    value\n",
    "  else // superstep > 0\n",
    "    (math.max(message, value._1), value._1) // return (newValue, oldValue)\n",
    "}\n",
    "\n",
    "// Scatter: the function for computing messages\n",
    "def sendMsg(triplet: EdgeTriplet[(Int, Int), Boolean]): Iterator[(VertexId, Int)] = {\n",
    "  val sourceVertex = triplet.srcAttr\n",
    "  if (sourceVertex._1 == sourceVertex._2) // newValue == oldValue for source vertex?\n",
    "    Iterator.empty // do nothing\n",
    "  else\n",
    "  // propogate new (updated) value to the destination vertex\n",
    "    Iterator((triplet.dstId, sourceVertex._1))\n",
    "}\n",
    "\n",
    "val minGraph = graph.pregel(initialMsg,\n",
    "  Int.MaxValue,\n",
    "  EdgeDirection.Out)(\n",
    "  vprog, // apply\n",
    "  sendMsg, // scatter\n",
    "  mergeMsg) // gather\n",
    "minGraph.vertices.collect.foreach {\n",
    "  case (vertexId, (value, original_value)) => println(value)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
